{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "operator/core.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.special import erfinv\n",
    "from scipy.stats import rankdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "import sys; sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace(torchtable, ..custom_types)\n",
    "from torchtable import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace(torchtable, .)\n",
    "from torchtable.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the core of torchtable is the Operator. We compose pipelines from Operators to process columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def _most_frequent(x: np.ndarray):\n",
    "    c = Counter(x)\n",
    "    return c.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operator:\n",
    "    \"\"\"\n",
    "    Base class for all operators.\n",
    "    Operators can be chained together by piping their outputs to new operators or hooking operators to other operators.\n",
    "    Any number of operators can be chained to become a pipeline, which is itself just another operator.\n",
    "    Subclasses should implement the `apply` method that defines the operation performed by the operator.\n",
    "    \n",
    "    Example:\n",
    "        >>> class TimesThree(Operator):\n",
    "        ...     def apply(self, x):\n",
    "        ...         return x * 3\n",
    "        >>> op = TimeThree()\n",
    "        >>> op(4) # 4 * 3 = 12\n",
    "        ... 12\n",
    "\n",
    "        >>> class Square(Operator):\n",
    "        ...     def apply(self, x):\n",
    "                    return x ** 2\n",
    "        >>> op = TimesThree() > Square()\n",
    "        >>> op(2) # (2 * 3) ** 2 = 36\n",
    "        ... 36\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.before = None\n",
    "        self.built = False\n",
    "\n",
    "    def __gt__(self, op: 'Operator') -> 'Operator':\n",
    "        \"\"\"Syntactic sugar for piping\"\"\"\n",
    "        return self.pipe(op)\n",
    "\n",
    "    def __lt__(self, op: 'Operator') -> 'Operator':\n",
    "        \"\"\"Syntactic sugar for hooking\"\"\"\n",
    "        return self.hook(op)\n",
    "    \n",
    "    def pipe(self, op: 'Operator') -> 'Operator':\n",
    "        \"\"\"Connect an operator after this operator. Returns the connected operator.\"\"\"\n",
    "        op.before = self\n",
    "        return op\n",
    "    \n",
    "    def hook(self, op: 'Operator') -> 'Operator':\n",
    "        \"\"\"Connect an operator to the *beginning* of this pipeline. Returns self.\"\"\"\n",
    "        if self.before is not None:\n",
    "            self.before.hook(op)\n",
    "        else:\n",
    "            self.before = op\n",
    "        return self\n",
    "\n",
    "    def apply(self, x: Any, train=True) -> Any:\n",
    "        \"\"\"\n",
    "        Takes output of previous stage in the pipeline and produces output. Override in subclasses.\n",
    "        Kwargs:\n",
    "            train: If true, this operator will \"train\" on the input. \n",
    "            In other words, the internal parameters of this operator may change to fit the given input.\n",
    "        \"\"\"\n",
    "        return x\n",
    "    \n",
    "    def __call__(self, x, **kwargs):\n",
    "        if self.before is not None:\n",
    "            return self.apply(self.before(x, **kwargs), **kwargs)\n",
    "        else:\n",
    "            return self.apply(x, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaOperator(Operator):\n",
    "    \"\"\"\n",
    "    Generic operator for stateless operation.\n",
    "    Args:\n",
    "        func: Function to apply to input.\n",
    "    \"\"\"\n",
    "    def __init__(self, func: Callable[[T], T]):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    \n",
    "    def apply(self, x: T, train=True) -> Any:\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerOperator(Operator):\n",
    "    \"\"\"\n",
    "    Wrapper for any stateful transformer with fit and transform methods.\n",
    "    Args:\n",
    "        transformer: Any object with a `fit` and `transform` method.\n",
    "    Example:\n",
    "        >>> op = TransformerOperator(sklearn.preprocessing.StandardScaler())\n",
    "    \"\"\"\n",
    "    def __init__(self, transformer):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer\n",
    "    \n",
    "    def build(self, x: Any) -> None:\n",
    "        self.transformer.fit(x)\n",
    "    \n",
    "    def apply(self, x: Any, train=True):\n",
    "        if train: self.build(x)\n",
    "        return self.transformer.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Normalizer:\n",
    "    _methods = set([\"Gaussian\", \"RankGaussian\"])\n",
    "\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "        if method is not None and method not in self._methods:\n",
    "            raise ValueError(f\"Invalid normalization method {method}\")\n",
    "    \n",
    "    def fit(self, x: pd.Series):\n",
    "        if self.method == \"Gaussian\":\n",
    "            self.mean, self.std = x.mean(), x.std()\n",
    "        elif self.method == \"RankGaussian\":\n",
    "            # TODO: store state\n",
    "            pass\n",
    "        return self\n",
    "\n",
    "    def transform(self, x: pd.Series) -> pd.Series:\n",
    "        if self.method == \"Gaussian\":\n",
    "            return (x - self.mean) / (self.std + 1e-8)\n",
    "        elif self.method == \"RankGaussian\":\n",
    "            # TODO: store state\n",
    "            # prevent divergence to infinity by restricting normalized ranks to range[-0.99, 0.99]\n",
    "            x = (rankdata(x) / len(x) - 0.5) * 0.99 * 2\n",
    "            x = erfinv(x)\n",
    "            return (x - x.mean())\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class Normalize(TransformerOperator):\n",
    "    \"\"\"\n",
    "    Normalizes a numeric field.\n",
    "    Args:\n",
    "        method: Method of normalization (choose from the following):\n",
    "        - None: No normalization will be applied (same as noop)\n",
    "        - 'Gaussian': Subtracts mean and divides by the standard deviation\n",
    "        - 'RankGaussian': Assigns elements to a Gaussian distribution based on their rank.\n",
    "    \"\"\"\n",
    "    def __init__(self, method: Optional[str]):\n",
    "        super().__init__(_Normalizer(method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _MissingFiller:\n",
    "    _method_mapping = {\n",
    "        \"median\": lambda x: x.median(),\n",
    "        \"mean\": lambda x: x.mean(),\n",
    "        \"mode\": lambda x: _most_frequent(x.dropna()),\n",
    "    }\n",
    "    \n",
    "    def __init__(self, method):\n",
    "        if callable(method):\n",
    "            self.method = method\n",
    "        elif method in self._method_mapping:\n",
    "            self.method = self._method_mapping[method]\n",
    "        elif method is None:\n",
    "            self.method = None\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid method of filling missing data: {method}\")\n",
    "        self.na_mapping = {}\n",
    "\n",
    "    def fit(self, x: pd.Series) -> '_MissingFiller':\n",
    "        if self.method is not None:\n",
    "            self.fill_value = self.method(x)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x: pd.Series) -> pd.Series:\n",
    "        if self.method is not None:\n",
    "            return x.fillna(self.fill_value)\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "class FillMissing(TransformerOperator):\n",
    "    \"\"\"\n",
    "    Fills missing values according to `method`\n",
    "    Args:\n",
    "        method: Method of filling missing values. Options:\n",
    "        - None: Do not fill missing values\n",
    "        - 'median': Fill with median\n",
    "        - 'mean': Fill with mean\n",
    "        - 'mode': Fill with mode. Effective for categorical fields.\n",
    "        - (any callable): The output of the callable will be used to fill the missing values\n",
    "    \"\"\"\n",
    "    def __init__(self, method: Union[Callable, str]):\n",
    "        super().__init__(_MissingFiller(method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"Mapping from category to integer id\"\"\"\n",
    "    def __init__(self, min_freq=0, max_features=None,\n",
    "                 handle_unk: Optional[bool]=False, nan_as_unk=False):\n",
    "        self.min_freq = min_freq\n",
    "        self.max_features = max_features\n",
    "        self.handle_unk = with_default(handle_unk, min_freq > 0 or max_features is not None)\n",
    "        self.nan_as_unk = nan_as_unk\n",
    "\n",
    "        if not self.handle_unk and (max_features is not None or min_freq > 0):\n",
    "            logger.warn(\"\"\"Setting max_features or min_freq will potentially cause some categories to become unknown.\n",
    "            Set handle_unk to True to handle categories left out due to max_features or min_freq being set.\n",
    "            \"\"\")\n",
    "        \n",
    "        if not handle_unk and nan_as_unk:\n",
    "            raise ValueError(\"\"\"Setting nan_as_unk=True requires the vocabulary to be able to handle unk.\n",
    "            Set handle_unk=True if setting nan_as_unk to True.\"\"\")\n",
    "    \n",
    "    def fit(self, x: pd.Series) -> 'Vocab':\n",
    "        \"\"\"Construct the mapping\"\"\"\n",
    "        counter = Counter()\n",
    "        for v in x:\n",
    "            if self.nan_as_unk and np.isnan(x): continue\n",
    "            counter[v] += 1\n",
    "        \n",
    "        self.index = defaultdict(int)\n",
    "        # if handle unknown category, reserve 0 for unseen categories\n",
    "        idx = 1 if self.handle_unk else 0\n",
    "        for k, c in counter.most_common(self.max_features):\n",
    "            if c < self.min_freq: break\n",
    "            self.index[k] = idx; idx += 1\n",
    "        return self\n",
    "    \n",
    "    def _get_index(self, x):\n",
    "        if x not in self.index and not self.handle_unk:\n",
    "            raise ValueError(\"Found category not in vocabulary. Try setting handle_unk to True.\")\n",
    "        else:\n",
    "            return self.index[x]\n",
    "    \n",
    "    def transform(self, x: pd.Series) -> pd.Series:\n",
    "        return x.apply(self._get_index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "    \n",
    "class Categorize(TransformerOperator):\n",
    "    \"\"\"\n",
    "    Converts categorical data into integer ids\n",
    "    Args:\n",
    "        min_freq: Minimum frequency required for a category to receive a unique id.\n",
    "        Any categories with a lower frequency will be treated as unknown categories.\n",
    "        max_features: Maximum number of unique categories to store. If larger than the number of actual categories,\n",
    "        the categories with the highest frequencies will be chosen. If None, there will be no limit on the number of categories.\n",
    "        handle_unk: Whether to allocate a unique id to unknown categories. \n",
    "        If you expect to see categories that you did not encounter in your training data, you should set this to True.\n",
    "        If None, handle_unk will be set to True if min_freq > 0 or max_features is not None, otherwise it will be False.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_freq: int=0, max_features: Optional[int]=None,\n",
    "                 handle_unk: Optional[bool]=None):\n",
    "        super().__init__(Vocab(min_freq=min_freq, max_features=max_features,\n",
    "                               handle_unk=handle_unk))\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(Operator):\n",
    "    \"\"\"\n",
    "    Convert input to a `torch.tensor`\n",
    "    Args:\n",
    "        dtype: The dtype of the output tensor\n",
    "    \"\"\"\n",
    "    def __init__(self, dtype: torch.dtype):\n",
    "        super().__init__()\n",
    "        self.dtype = dtype\n",
    "    \n",
    "    def apply(self, x: ArrayLike, device: Optional[torch.device]=None, train=True) -> torch.tensor:\n",
    "        arr = to_numpy_array(x)\n",
    "        # convert dtype to PyTorch compatible type\n",
    "        if arr.dtype == np.bool_:\n",
    "            arr = arr.astype(\"int\")\n",
    "        return torch.tensor(arr, dtype=self.dtype, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_operator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtable import *\n",
    "from torchtable.operator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pipe\n",
    "op1 = LambdaOperator(lambda x: x + 1)\n",
    "op2 = op1 > LambdaOperator(lambda x: x ** 2)\n",
    "assert op2(1) == 4\n",
    "op3 = LambdaOperator(lambda x: x + 3)\n",
    "op2 > op3\n",
    "assert op3(2) == 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_hook\n",
    "op1 = LambdaOperator(lambda x: x + 3)\n",
    "op2 = LambdaOperator(lambda x: x * 2)\n",
    "op2 < op1\n",
    "assert op2(1) == 8\n",
    "op3 = LambdaOperator(lambda x: x ** 2)\n",
    "op3 < op2\n",
    "assert op3(1) == 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_normalizer_gaussian\n",
    "norm = Normalize(\"Gaussian\")\n",
    "rng = np.random.RandomState(21)\n",
    "a = rng.normal(4, 10, (200, ))\n",
    "a_normed = norm(a)\n",
    "np.testing.assert_almost_equal(a_normed.mean(), 0.)\n",
    "np.testing.assert_almost_equal(a_normed.std(), 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_normalizer_rank_gaussian\n",
    "norm = Normalize(\"RankGaussian\")\n",
    "rng = np.random.RandomState(21)\n",
    "a = rng.normal(4, 10, (200, ))\n",
    "a_normed = norm(a)\n",
    "np.testing.assert_almost_equal(a_normed.mean(), 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_missing_filler\n",
    "rng = np.random.RandomState(21)\n",
    "x = pd.Series(data=rng.normal(0, 1, (100, )))\n",
    "x[x < 0] = np.nan\n",
    "for mthd in [\"median\", \"mean\", \"mode\"]:\n",
    "    filler = FillMissing(mthd)\n",
    "    assert not pd.isnull(filler(x)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_categorize\n",
    "rng = np.random.RandomState(21)\n",
    "a = pd.Series(data=rng.randint(0, 20, (100, )))\n",
    "cat = Categorize()\n",
    "a_transformed = cat(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_categorize_min_max_freq\n",
    "rng = np.random.RandomState(21)\n",
    "a = pd.Series(data=np.array([1, 2, 1, 4, 1, 2, 3, 3, 5]))\n",
    "cat = Categorize(min_freq=2, max_features=None, handle_unk=True)\n",
    "a_transformed = cat(a)\n",
    "assert (a_transformed[a == 4] == 0).all()\n",
    "assert (a_transformed[a == 5] == 0).all()\n",
    "assert (a_transformed[a == 1] != 0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_categorize_unknown\n",
    "rng = np.random.RandomState(21)\n",
    "a = pd.Series(data=np.array([0, 6, 7, 8, 9, 6, 3, 1, 2, 4]))\n",
    "cat = Categorize(min_freq=0, max_features=None, handle_unk=True)\n",
    "cat(pd.Series(data=np.arange(6)))\n",
    "a_transformed = cat(a, train=False)\n",
    "assert (a_transformed[a > 5] == 0).all()\n",
    "assert (a_transformed[a <= 5] > 0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_to_tensor\n",
    "\"\"\"Smoke test for ToTensor\"\"\"\n",
    "rng = np.random.RandomState(21)\n",
    "a = pd.Series(data=rng.normal(0, 1, (100, )))\n",
    "to_tsr = ToTensor(torch.float)\n",
    "tsr = to_tsr(a, device=None)\n",
    "tsr = to_tsr(a.values, device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_to_tensor_bool\n",
    "\"\"\"Smoke test for ToTensor with boolean inputs\"\"\"\n",
    "x = pd.Series(data=np.array([True, False, True, False]))\n",
    "to_tsr = ToTensor(torch.long)\n",
    "tsr = to_tsr(x, device=None)\n",
    "tsr = to_tsr(x.values, device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
