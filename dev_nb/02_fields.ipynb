{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "field/core.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.utils.data\n",
    "from pathlib import Path\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "import sys; sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace(torchtable, ..custom_types)\n",
    "from torchtable import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace(torchtable, .)\n",
    "from torchtable.utils import *\n",
    "from torchtable.operator import Operator, LambdaOperator, FillMissing, Categorize, Normalize, ToTensor, UnknownCategoryError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Field:\n",
    "    \"\"\"\n",
    "    A single field in the output mini batch. A Field acts as a continaer for all relevant information regarding an output in the output mini batch.\n",
    "    Primarily, it stores a pipeline to apply to a column/set of columns in the input.\n",
    "    It also stores a pipeline for converting the input batch to an appropriate type for the downstream model (generally a torch.tensor).\n",
    "    This class can directly be instantiated with a custom pipeline but is generally used as a subclass for other fields.\n",
    "\n",
    "    Example:\n",
    "        >>> fld = Field(LambdaOperator(lambda x: x + 1) > LambdaOperator(lambda x: x ** 2))\n",
    "        >>> fld.transform(1)\n",
    "        ... 9\n",
    "    \n",
    "    Args:\n",
    "        pipeline: An operator representing the set of operations mapping the input column to the output.\n",
    "            This transformation will be applied during the construction of the dataset. \n",
    "            If the pipeline is resource intensive and applying it all at once is unrealistic, consider deferring some of the processing to `batch_pipeline`.\n",
    "        is_target: Whether the field is an input or target field. Affects default batching behavior.\n",
    "        continuous: Whether the output is continuous.\n",
    "        categorical: Whether the output is categorical/discrete.\n",
    "        batch_pipeline: The transformation to apply to this field during batching.\n",
    "            By default, this will simply be an operation to transform the input to a tensor to feed to the model.\n",
    "            This can be set to any Operator that the user wishes so that arbitrary transformations (e.g. padding, noising) can be applied during data loading.\n",
    "        dtype: The output tensor dtype. Only relevant when batch_pipeline is None (using the default pipeline).\n",
    "        metadata: Additional data about the field to store. \n",
    "            Use cases include adding data about model parameters (e.g. size of embeddings for this field).\n",
    "    \"\"\"\n",
    "    def __init__(self, pipeline: Operator, name: Optional[str]=None,\n",
    "                 is_target: bool=False, continuous: bool=True,\n",
    "                 categorical: bool=False, batch_pipeline: Optional[Operator]=None,\n",
    "                 dtype: Optional[torch.dtype]=None, metadata: dict={}):\n",
    "        self.pipeline = pipeline\n",
    "        self.name = name\n",
    "        self.is_target = is_target\n",
    "        if categorical and continuous:\n",
    "            raise ValueError(\"\"\"A field cannot be both continuous and categorical. \n",
    "            If you want both a categorical and continuous representation, consider using multiple fields.\"\"\")\n",
    "        self.continuous, self.categorical = continuous, categorical\n",
    "\n",
    "        if dtype is not None and batch_pipeline is not None:\n",
    "            logger.warning(\"\"\"Setting a custom batch pipeline will cause this field to ignore the dtype argument.\n",
    "            If you want to manually set the dtype, consider attaching a ToTensor operation to the pipeline.\"\"\")\n",
    "        dtype = with_default(dtype, torch.long if self.categorical else torch.float)\n",
    "        self.batch_pipeline = with_default(batch_pipeline, ToTensor(dtype))\n",
    "        self.metadata = metadata\n",
    "        \n",
    "    def transform(self, x: pd.Series, train=True) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Method to process the input column during construction of the dataset.\n",
    "        Kwargs:\n",
    "            train: If true, this transformation may change some internal parameters of the pipeline.\n",
    "                For instance, if there is a normalization step in the pipeline, \n",
    "                the mean and std will be computed on the current input.\n",
    "                Otherwise, the pipeline will use statistics computed in the past.\n",
    "        \"\"\"\n",
    "        return self.pipeline(x, train=train)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}[{self.name}]\"\n",
    "    \n",
    "    def transform_batch(self, x: ArrayLike, device: Optional[torch.device]=None, \n",
    "                        train: bool=True) -> torch.tensor:\n",
    "        \"\"\"Method to process batch input during loading of the dataset.\"\"\"\n",
    "        return self.batch_pipeline(x, device=device, train=train)\n",
    "\n",
    "    def index(self, example: ArrayLike, idx) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Wrapper for indexing. The field must provide the ability to index via a list for batching later on.\n",
    "        \"\"\"\n",
    "        if isinstance(example, pd.Series):\n",
    "            return example.iloc[idx]\n",
    "        else:\n",
    "            return example[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityField(Field):\n",
    "    \"\"\"\n",
    "    A field that does not modify the input.\n",
    "    \"\"\"\n",
    "    def __init__(self, name=None, is_target=False, continuous=True, categorical=False, metadata={}):\n",
    "        super().__init__(LambdaOperator(lambda x: x), name=name,\n",
    "                         is_target=is_target, continuous=continuous, categorical=categorical, metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericField(Field):\n",
    "    \"\"\"\n",
    "    A field corresponding to a continous, numerical output (e.g. price, distance, etc.)\n",
    "    \n",
    "    Args:\n",
    "        fill_missing: The method of filling missing values. See the `FillMissing` operator for details.\n",
    "        \n",
    "        normalization: The method of normalization. See the `Normalize` operator for details.\n",
    "    \"\"\"\n",
    "    def __init__(self, name=None,\n",
    "                 fill_missing=\"median\", normalization=\"Gaussian\",\n",
    "                 is_target=False, metadata={}):\n",
    "        pipeline = FillMissing(fill_missing) > Normalize(normalization)\n",
    "        super().__init__(pipeline, name, is_target, continuous=True, categorical=False, metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalField(Field):\n",
    "    \"\"\"\n",
    "    A field corresponding to a categorica, discrete output (e.g. id, group, gender)\n",
    "    \n",
    "    Args:\n",
    "        See the `Categorize` operator for more details.\n",
    "    \"\"\"\n",
    "    def __init__(self, name=None, min_freq=0, max_features=None,\n",
    "                 handle_unk=None, is_target=False, metadata: dict={}):\n",
    "        pipeline = Categorize(min_freq=min_freq, max_features=max_features,\n",
    "                              handle_unk=handle_unk)\n",
    "        self.vocab = pipeline.transformer\n",
    "        super().__init__(pipeline, name, is_target, continuous=False, categorical=True, metadata=metadata)\n",
    "    \n",
    "    def transform(self, x: pd.Series, train=True) -> ArrayLike:\n",
    "        try:\n",
    "            return super().transform(x, train=train)\n",
    "        except UnknownCategoryError:\n",
    "            raise UnknownCategoryError(f\"Unknown category encountered in {self.name}. Consider setting handle_unk=True.\")\n",
    "    \n",
    "    @property\n",
    "    def cardinality(self):\n",
    "        \"\"\"The number of unique outputs.\"\"\"\n",
    "        return len(self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "class DatetimeFeatureField(Field):\n",
    "    \"\"\"\n",
    "    A generic field for constructing features from datetime columns.\n",
    "    \n",
    "    Args:\n",
    "        func: Feature construction function\n",
    "    \"\"\"\n",
    "    def __init__(self, func: Callable[[pd.Series], pd.Series], fill_missing: Optional[str]=None,\n",
    "                 name=None, is_target=False, continuous=False, metadata: dict={}):\n",
    "        pipeline = (LambdaOperator(lambda s: pd.to_datetime(s))\n",
    "                    > FillMissing(method=fill_missing) \n",
    "                    > LambdaOperator(lambda s: func(s.dt)))\n",
    "        super().__init__(pipeline, name=name, is_target=is_target, continuous=continuous,\n",
    "                         categorical=not continuous, metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "class DayofWeekField(DatetimeFeatureField):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(lambda x: x.dayofweek, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "class DayField(DatetimeFeatureField):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(lambda x: x.day, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "class MonthStartField(DatetimeFeatureField):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(lambda x: x.is_month_start, continuous=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "class MonthEndField(DatetimeFeatureField):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(lambda x: x.is_month_end, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "class HourField(DatetimeFeatureField):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(lambda x: x.hour, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "def date_fields(**kwargs) -> List[DatetimeFeatureField]:\n",
    "    \"\"\"The default set of fields for feature engineering using a field with date information\"\"\"\n",
    "    return [DayofWeekField(**kwargs), DayField(**kwargs),\n",
    "            MonthStartField(**kwargs), MonthEndField(**kwargs),\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "def datetime_fields(**kwargs) -> List[DatetimeFeatureField]:\n",
    "    \"\"\"The default set of fields for feature engineering using a field with date and time information\"\"\"\n",
    "    return [DayofWeekField(**kwargs), DayField(**kwargs),\n",
    "            MonthStartField(**kwargs), MonthEndField(**kwargs),\n",
    "            HourField(**kwargs),\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FieldCollection(list):\n",
    "    \"\"\"\n",
    "    A list of fields with some auxillary methods.\n",
    "    \n",
    "    Args:\n",
    "        flatten: If set to True, each field in this collection will be mapped to one key in the batch/dataset.\n",
    "            Otherwise, each field in this collection will be mapped to an entry in a list for the same key in the batch/dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, flatten: bool=False, namespace: Optional[str]=None):\n",
    "        for a in args: self.append(a)\n",
    "        self.flatten = flatten\n",
    "        self.namespace = None\n",
    "        self.set_namespace(namespace)\n",
    "    \n",
    "    def index(self, examples: List[ArrayLike], idx) -> List[ArrayLike]:\n",
    "        return [fld.index(ex, idx) for fld, ex in zip(self, examples)]\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self.namespace\n",
    "\n",
    "    def set_namespace(self, nm: str) -> None:\n",
    "        \"\"\"Set names of inner fields as well\"\"\"\n",
    "        old_namespace = self.namespace\n",
    "        if old_namespace == nm: return\n",
    "        self.namespace = nm\n",
    "        for i, fld in enumerate(self):\n",
    "            if fld.name is None: \n",
    "                fld.name = f\"{self.namespace}/_{i}\"\n",
    "            else:\n",
    "                if old_namespace is not None and fld.name.startswith(f\"{old_namespace}/\"):\n",
    "                    fld.name = fld.name[len(old_namespace)+1:]\n",
    "                fld.name = f\"{self.namespace}/{fld.name}\"    \n",
    "    @name.setter\n",
    "    def name(self, nm: str):\n",
    "        self.set_namespace(nm)\n",
    "    \n",
    "    def transform(self, *args, **kwargs) -> list:\n",
    "        \"\"\"Applies transform with each field and returns a list\"\"\"\n",
    "        return [fld.transform(*args, **kwargs) for fld in self]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_field.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "from torchtable import *\n",
    "from torchtable.operator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment\n",
    "# from torchtable import *\n",
    "# from torchtable.operator import *\n",
    "# from torchtable.field import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_field\n",
    "fld = Field(LambdaOperator(lambda x: x + 1) > LambdaOperator(lambda x: x ** 2))\n",
    "assert fld.transform(1) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_numeric_field\n",
    "rng = np.random.RandomState(21)\n",
    "x = pd.Series(data=rng.normal(0, 1, (100, )))\n",
    "x[x < 0] = np.nan\n",
    "for mthd in [\"median\", \"mean\", \"mode\"]:\n",
    "    fld = NumericField(fill_missing=mthd)\n",
    "    assert not pd.isnull(fld.transform(x)).any()\n",
    "\n",
    "fld = NumericField(fill_missing=None)\n",
    "assert pd.isnull(fld.transform(x)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_numeric_field_norm\n",
    "rng = np.random.RandomState(21)\n",
    "x = pd.Series(data=rng.normal(-1, 4, (100, )))\n",
    "fld = NumericField(fill_missing=None, normalization=\"Gaussian\")\n",
    "np.testing.assert_almost_equal(fld.transform(x).mean(), 0.)\n",
    "np.testing.assert_almost_equal(fld.transform(x).std(), 1.)\n",
    "\n",
    "fld = NumericField(fill_missing=None, normalization=\"RankGaussian\")\n",
    "np.testing.assert_almost_equal(fld.transform(x).mean(), 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_numeric_joint\n",
    "\"\"\"Smoke test for NumericField with various settings\"\"\"\n",
    "rng = np.random.RandomState(21)\n",
    "x = pd.Series(data=rng.normal(2, 0.5, (100, )))\n",
    "for fill_mthd in [\"median\", \"mean\", \"mode\"]:\n",
    "    for norm_mthd in [None, \"Gaussian\", \"RankGaussian\"]:\n",
    "        fld = NumericField(fill_missing=fill_mthd, normalization=norm_mthd)\n",
    "        fld.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_categorical_field\n",
    "\"\"\"Smoke test for categorical field with default settings\"\"\"\n",
    "rng = np.random.RandomState(21)\n",
    "x = pd.Series(data=rng.randint(-3, 15, (100, )))\n",
    "fld = CategoricalField(handle_unk=False)\n",
    "assert fld.transform(x).nunique() == len(fld.vocab)\n",
    "assert fld.transform(x).nunique() == fld.cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_datetime_fields\n",
    "\"\"\"Smoke test for fields\"\"\"\n",
    "x = pd.to_datetime(pd.DataFrame({'year': [2015, 2016, 2015, 2017, 2020], 'month': [2, 3, 4, 5, 1], \n",
    "                             'day': [4, 5, 10, 29, 30], 'hour': [2, 3, 12, 11, 5]}))\n",
    "for fld_type in [DayofWeekField, DayField, MonthStartField, MonthEndField, HourField]:\n",
    "    assert not pd.isnull(fld_type().transform(x)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_date_fields\n",
    "x = pd.to_datetime(pd.DataFrame({'year': [2011, 1995, 2015, 2017, 2030], 'month': [12, 9, 7, 5, 10], \n",
    "                                 'day': [14, 13, 9, 19, 1]}))\n",
    "for fld_type in [DayofWeekField, DayField, MonthStartField, MonthEndField]:\n",
    "    assert not pd.isnull(fld_type().transform(x)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_batch_transform\n",
    "\"\"\"Smoke test for batch transformations\"\"\"\n",
    "rng = np.random.RandomState(21)\n",
    "a = pd.Series(data=rng.normal(0, 1, (100, )))\n",
    "fld = NumericField()\n",
    "tsr = fld.transform_batch(fld.transform(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_field_metadata\n",
    "fld = NumericField(metadata={\"foo\": \"bar\"})\n",
    "assert fld.metadata[\"foo\"] == \"bar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_field_collection\n",
    "fld0 = NumericField()\n",
    "fld1 = CategoricalField()\n",
    "flds = FieldCollection(fld0, fld1)\n",
    "assert len(flds) == 2\n",
    "assert flds[0] == fld0\n",
    "assert flds[1] == fld1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_namespace\n",
    "fld0 = NumericField()\n",
    "fld1 = CategoricalField(name=\"bar\")\n",
    "flds = FieldCollection(fld0, fld1, namespace=\"foo\")\n",
    "assert fld0.name == \"foo/_0\"\n",
    "assert fld1.name == \"foo/bar\"\n",
    "flds.name = \"hoge\"\n",
    "assert fld0.name == \"hoge/_0\"\n",
    "assert fld1.name == \"hoge/bar\"\n",
    "\n",
    "fld0 = NumericField()\n",
    "fld1 = CategoricalField(name=\"bar\")\n",
    "flds = FieldCollection(fld0, fld1)\n",
    "flds.name = \"hoge\"\n",
    "flds.name = \"hoge\"\n",
    "assert fld0.name == \"hoge/_0\"\n",
    "assert fld1.name == \"hoge/bar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_index\n",
    "fld = NumericField()\n",
    "np.testing.assert_almost_equal(fld.index(np.arange(10), [0, 3, 5]), np.array([0, 3, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_index_series\n",
    "fld = NumericField()\n",
    "np.testing.assert_almost_equal(fld.index(pd.Series(data=np.arange(10)), [0, 3, 5]), np.array([0, 3, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_index_fieldcollection\n",
    "flds = FieldCollection(NumericField(), NumericField())\n",
    "arr1, arr2 = flds.index([np.array([5, 4, 2, 3, 1]), np.array([1, 2, 3, 4, 5])], [1, 4, 2])\n",
    "np.testing.assert_almost_equal(arr1, np.array([4, 1, 2]))\n",
    "np.testing.assert_almost_equal(arr2, np.array([2, 5, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_fieldcollection_transform\n",
    "flds = FieldCollection(Field(LambdaOperator(lambda x: x * 2)), Field(LambdaOperator(lambda x: x + 3)))\n",
    "assert flds.transform(1) == [2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_unknown_cat\n",
    "fld = CategoricalField(name=\"hoge\", handle_unk=False)\n",
    "a = pd.Series(data=np.array([1, 2, 3]))\n",
    "fld.transform(a)\n",
    "b = pd.Series(data=np.array([3, 4]))\n",
    "with pytest.raises(UnknownCategoryError):\n",
    "    fld.transform(b, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
