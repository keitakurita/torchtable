{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "field/core.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.utils.data\n",
    "from pathlib import Path\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "import sys; sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace(torchtable, ..custom_types)\n",
    "from torchtable import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace(torchtable, .)\n",
    "from torchtable.utils import *\n",
    "from torchtable.operator import Operator, LambdaOperator, FillMissing, Categorize, Normalize, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Field:\n",
    "    \"\"\"\n",
    "    A single field in the output mini batch. \n",
    "    A Field object wraps a pipeline to apply to a column/set of columns in the input.\n",
    "    This class can directly be instantiated with a custom pipeline.\n",
    "    Example:\n",
    "        >>> fld = Field(LambdaOperator(lambda x: x + 1) > LambdaOperator(lambda x: x ** 2))\n",
    "        >>> fld.transform(1)\n",
    "        ... 9\n",
    "    Args:\n",
    "        pipeline: An operator representing the set of operations mapping the input column to the output.\n",
    "        This transformation will be applied during the construction of the dataset. \n",
    "        If the pipeline is resource intensive and applying it all at once is unrealistic, consider deferring some of the processing to `batch_pipeline`.\n",
    "    Kwargs:\n",
    "        is_target: Whether the field is an input or target field. Affects default batching behavior.\n",
    "        continuous: Whether the output is continuous.\n",
    "        categorical: Whether the output is categorical/discrete.\n",
    "        batch_pipeline: The transformation to apply to this field during batching.\n",
    "        By default, this will simply be an operation to transform the input to a tensor to feed to the model.\n",
    "        This can be set to any Operator that the user wishes so that arbitrary transformations (e.g. padding, noising) can be applied during data loading.\n",
    "        dtype: The output tensor dtype. Only relevant when batch_pipeline is None (using the default pipeline).\n",
    "    \"\"\"\n",
    "    def __init__(self, pipeline: Operator, name: Optional[str]=None,\n",
    "                 is_target: bool=False, continuous: bool=True,\n",
    "                 categorical: bool=False, batch_pipeline: Optional[Operator]=None,\n",
    "                 dtype: Optional[torch.dtype]=None):\n",
    "        self.pipeline = pipeline\n",
    "        self.name = name\n",
    "        self.is_target = is_target\n",
    "        if categorical and continuous:\n",
    "            raise ValueError(\"\"\"A field cannot be both continuous and categorical. \n",
    "            If you want both a categorical and continuous representation, consider using multiple fields.\"\"\")\n",
    "        self.continuous = continuous\n",
    "        self.categorical = categorical\n",
    "        if dtype is not None and batch_pipeline is not None:\n",
    "            logger.warning(\"\"\"Setting a custom batch pipeline will cause this field to ignore the dtype argument.\n",
    "            If you want to manually set the dtype, consider attaching a ToTensor operation to the pipeline.\"\"\")\n",
    "        dtype = with_default(dtype, torch.long if self.categorical else torch.float)\n",
    "        self.batch_pipeline = with_default(batch_pipeline, ToTensor(dtype))\n",
    "        \n",
    "    def transform(self, x: pd.Series, train=True) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Method to process the input column during construction of the dataset.\n",
    "        Kwargs:\n",
    "            train: If true, this transformation may change some internal parameters of the pipeline.\n",
    "            For instance, if there is a normalization step in the pipeline, the mean and std will be computed on the current input.\n",
    "            Otherwise, the pipeline will use statistics computed in the past.\n",
    "        \"\"\"\n",
    "        return self.pipeline(x, train=train)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}[{self.name}]\"\n",
    "    \n",
    "    def transform_batch(self, x: ArrayLike, device: Optional[torch.device]=None, \n",
    "                        train: bool=True) -> torch.tensor:\n",
    "        \"\"\"Method to process batch input during loading of the dataset.\"\"\"\n",
    "        return self.batch_pipeline(x, device=device, train=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityField(Field):\n",
    "    \"\"\"\n",
    "    A field that does not modify the input.\n",
    "    \"\"\"\n",
    "    def __init__(self, name=None, is_target=False, continuous=True, categorical=False):\n",
    "        super().__init__(LambdaOperator(lambda x: x), name=name,\n",
    "                         is_target=is_target, continuous=continuous, categorical=categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericField(Field):\n",
    "    \"\"\"\n",
    "    A field corresponding to a continous, numerical output (e.g. price, distance, etc.)\n",
    "    Args:\n",
    "        fill_missing: The method of filling missing values. See the `FillMissing` operator for details.\n",
    "        normalization: The method of normalization. See the `Normalize` operator for details.\n",
    "    \"\"\"\n",
    "    def __init__(self, name=None,\n",
    "                 fill_missing=\"median\", normalization=\"Gaussian\",\n",
    "                 is_target=False):\n",
    "        pipeline = FillMissing(fill_missing) > Normalize(normalization)\n",
    "        super().__init__(pipeline, name, is_target, continuous=True, categorical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalField(Field):\n",
    "    \"\"\"\n",
    "    A field corresponding to a categorica, discrete output (e.g. id, group, gender)\n",
    "    Args:\n",
    "        See the `Categorize` operator for more details.\n",
    "    \"\"\"\n",
    "    def __init__(self, name=None, min_freq=0, max_features=None,\n",
    "                 handle_unk=None, is_target=False):\n",
    "        pipeline = Categorize(min_freq=min_freq, max_features=max_features,\n",
    "                              handle_unk=handle_unk)\n",
    "        self.vocab = pipeline.transformer\n",
    "        super().__init__(pipeline, name, is_target, continuous=False, categorical=True)\n",
    "    \n",
    "    @property\n",
    "    def cardinality(self):\n",
    "        \"\"\"The number of unique outputs.\"\"\"\n",
    "        return len(self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatetimeFeatureField(Field):\n",
    "    \"\"\"\n",
    "    A generic field for constructing features from datetime columns.\n",
    "    Args:\n",
    "        func: Feature construction function\n",
    "    \"\"\"\n",
    "    def __init__(self, func: Callable[[pd.Series], pd.Series], fill_missing: Optional[str]=None,\n",
    "                 name=None, is_target=False, continuous=False):\n",
    "        pipeline = (LambdaOperator(lambda s: pd.to_datetime(s))\n",
    "                    > FillMissing(method=fill_missing) \n",
    "                    > LambdaOperator(lambda s: func(s.dt)))\n",
    "        super().__init__(pipeline, name=name, is_target=is_target, continuous=continuous, categorical=not continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DayofWeekField(DatetimeFeatureField):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(lambda x: x.dayofweek, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DayField(DatetimeFeatureField):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(lambda x: x.day, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonthStartField(DatetimeFeatureField):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(lambda x: x.is_month_start, continuous=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonthEndField(DatetimeFeatureField):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(lambda x: x.is_month_end, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HourField(DatetimeFeatureField):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(lambda x: x.hour, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_fields(**kwargs) -> List[DatetimeFeatureField]:\n",
    "    \"\"\"The default set of fields for feature engineering using a field with date information\"\"\"\n",
    "    return [DayofWeekField(**kwargs), DayField(**kwargs),\n",
    "            MonthStartField(**kwargs), MonthEndField(**kwargs),\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_fields(**kwargs) -> List[DatetimeFeatureField]:\n",
    "    \"\"\"The default set of fields for feature engineering using a field with date and time information\"\"\"\n",
    "    return [DayofWeekField(**kwargs), DayField(**kwargs),\n",
    "            MonthStartField(**kwargs), MonthEndField(**kwargs),\n",
    "            HourField(**kwargs),\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_field.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtable import *\n",
    "from torchtable.operator import *\n",
    "from torchtable.field import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_field\n",
    "fld = Field(LambdaOperator(lambda x: x + 1) > LambdaOperator(lambda x: x ** 2))\n",
    "assert fld.transform(1) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_numeric_field\n",
    "rng = np.random.RandomState(21)\n",
    "x = pd.Series(data=rng.normal(0, 1, (100, )))\n",
    "x[x < 0] = np.nan\n",
    "for mthd in [\"median\", \"mean\", \"mode\"]:\n",
    "    fld = NumericField(fill_missing=mthd)\n",
    "    assert not pd.isnull(fld.transform(x)).any()\n",
    "\n",
    "fld = NumericField(fill_missing=None)\n",
    "assert pd.isnull(fld.transform(x)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_numeric_field_norm\n",
    "rng = np.random.RandomState(21)\n",
    "x = pd.Series(data=rng.normal(-1, 4, (100, )))\n",
    "fld = NumericField(fill_missing=None, normalization=\"Gaussian\")\n",
    "np.testing.assert_almost_equal(fld.transform(x).mean(), 0.)\n",
    "np.testing.assert_almost_equal(fld.transform(x).std(), 1.)\n",
    "\n",
    "fld = NumericField(fill_missing=None, normalization=\"RankGaussian\")\n",
    "np.testing.assert_almost_equal(fld.transform(x).mean(), 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_numeric_joint\n",
    "\"\"\"Smoke test for NumericField with various settings\"\"\"\n",
    "rng = np.random.RandomState(21)\n",
    "x = pd.Series(data=rng.normal(2, 0.5, (100, )))\n",
    "for fill_mthd in [\"median\", \"mean\", \"mode\"]:\n",
    "    for norm_mthd in [None, \"Gaussian\", \"RankGaussian\"]:\n",
    "        fld = NumericField(fill_missing=fill_mthd, normalization=norm_mthd)\n",
    "        fld.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_categorical_field\n",
    "\"\"\"Smoke test for categorical field with default settings\"\"\"\n",
    "rng = np.random.RandomState(21)\n",
    "x = pd.Series(data=rng.randint(-3, 15, (100, )))\n",
    "fld = CategoricalField(handle_unk=False)\n",
    "assert fld.transform(x).nunique() == len(fld.vocab)\n",
    "assert fld.transform(x).nunique() == fld.cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_datetime_fields\n",
    "\"\"\"Smoke test for fields\"\"\"\n",
    "x = pd.to_datetime(pd.DataFrame({'year': [2015, 2016, 2015, 2017, 2020], 'month': [2, 3, 4, 5, 1], \n",
    "                             'day': [4, 5, 10, 29, 30], 'hour': [2, 3, 12, 11, 5]}))\n",
    "for fld_type in [DayofWeekField, DayField, MonthStartField, MonthEndField, HourField]:\n",
    "    assert not pd.isnull(fld_type().transform(x)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_date_fields\n",
    "x = pd.to_datetime(pd.DataFrame({'year': [2011, 1995, 2015, 2017, 2030], 'month': [12, 9, 7, 5, 10], \n",
    "                                 'day': [14, 13, 9, 19, 1]}))\n",
    "for fld_type in [DayofWeekField, DayField, MonthStartField, MonthEndField]:\n",
    "    assert not pd.isnull(fld_type().transform(x)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1459, -0.2012,  0.8748, -1.2703,  0.5982, -1.6943, -0.2896, -0.3164,\n",
       "         0.9554, -0.1092, -0.6697,  1.1846,  1.4060, -0.7407,  0.5483, -0.5157,\n",
       "         0.0541,  0.1426, -1.2878, -0.0377, -1.0885, -1.0208, -0.5246, -1.9492,\n",
       "        -1.4754,  0.1189,  0.5037,  0.6978, -0.5305,  0.5532,  1.3658,  1.0025,\n",
       "        -1.3891, -1.7282,  0.2784,  1.0640, -0.1197,  0.7915,  0.1312,  0.1117,\n",
       "         0.1794,  0.1086, -0.4925,  1.6250,  0.7613, -0.6181, -0.3636, -1.0692,\n",
       "         0.3509, -1.4296, -0.4085,  0.6000,  0.4254,  0.4380, -1.1995,  0.6162,\n",
       "        -0.2229,  1.1388, -0.3554,  1.4700,  0.2900,  0.6003, -0.4638, -0.4767,\n",
       "        -0.4576,  0.8367, -0.5153, -0.2058,  0.9936,  0.3838, -0.3381,  0.1686,\n",
       "        -1.4083, -1.5058,  0.3079,  2.4737,  1.7792,  2.2543,  0.4349, -0.5888,\n",
       "         0.8995, -1.2123, -2.7663,  0.2984,  0.8672,  0.5607,  1.3494,  0.6169,\n",
       "         0.2257, -1.0809, -0.2183, -1.2000, -0.4417,  1.6783,  0.0987,  0.2541,\n",
       "        -0.7451,  2.3317, -1.9484, -0.9625])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_batch_transform\n",
    "\"\"\"Smoke test for batch transformations\"\"\"\n",
    "rng = np.random.RandomState(21)\n",
    "a = pd.Series(data=rng.normal(0, 1, (100, )))\n",
    "fld = NumericField()\n",
    "fld.transform_batch(fld.transform(a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
