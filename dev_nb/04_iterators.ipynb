{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loader/core.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from contextlib import contextmanager\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "import sys; sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace(torchtable, ..custom_types)\n",
    "from torchtable import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace(torchtable, .)\n",
    "from torchtable.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomShuffler(object):\n",
    "    \"\"\"\n",
    "    Use random functions while keeping track of the random state to make it\n",
    "    reproducible and deterministic. Borrowed from torchtext.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, random_state=None):\n",
    "        self._random_state = random_state\n",
    "        if self._random_state is None:\n",
    "            self._random_state = random.getstate()\n",
    "\n",
    "    @contextmanager\n",
    "    def use_internal_state(self):\n",
    "        \"\"\"Use a specific RNG state.\"\"\"\n",
    "        old_state = random.getstate()\n",
    "        random.setstate(self._random_state)\n",
    "        yield\n",
    "        self._random_state = random.getstate()\n",
    "        random.setstate(old_state)\n",
    "\n",
    "    @property\n",
    "    def random_state(self):\n",
    "        return deepcopy(self._random_state)\n",
    "\n",
    "    @random_state.setter\n",
    "    def random_state(self, s):\n",
    "        self._random_state = s\n",
    "\n",
    "    def __call__(self, data):\n",
    "        \"\"\"Shuffle and return a new list.\"\"\"\n",
    "        with self.use_internal_state():\n",
    "            return random.sample(data, len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcessedBatch = Tuple[Dict[ColumnName, OneorMore[torch.tensor]], Dict[ColumnName, OneorMore[torch.tensor]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultLoader(torch.utils.data.DataLoader):\n",
    "    \"\"\"\n",
    "    Defines an iterator that loads batches of data from a Dataset.\n",
    "    Heavily based on the Iterator from torchtext.\n",
    "\n",
    "    Args:\n",
    "        dataset: The Dataset object to load examples from.\n",
    "        batch_size: Batch size.\n",
    "        repeat: Whether to repeat the iterator for multiple epochs.\n",
    "        shuffle: Whether to shuffle examples between epochs.\n",
    "        device (str or `torch.device`): A string or instance of `torch.device`\n",
    "            specifying which device the Variables are going to be created on.\n",
    "            If None, the tensors will be created on cpu.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: torch.utils.data.Dataset, batch_size: int,\n",
    "                 device: Optional[torch.device]=None, repeat: bool=False,\n",
    "                 shuffle: Optional[bool]=None):\n",
    "        self.batch_size, self.dataset = batch_size, dataset\n",
    "        self.iterations = 0\n",
    "        self.repeat = repeat\n",
    "        self.shuffle = with_default(shuffle, self.dataset.train)\n",
    "\n",
    "        if isinstance(device, int):\n",
    "            warnings.warn(\"The `device` argument should be set by using `torch.device`\" +\n",
    "                           \" or passing a string as an argument. This behavior will be\" +\n",
    "                           \" deprecated soon and currently defaults to cpu.\")\n",
    "            device = None\n",
    "        self.device = device\n",
    "        if self.shuffle:\n",
    "            # TODO: Clean interface\n",
    "            self.index_generator = RandomShuffler()\n",
    "        else:\n",
    "            self.index_generator = lambda x: x\n",
    "\n",
    "        # For state loading/saving only\n",
    "        self._iterations_this_epoch = 0\n",
    "        self._random_state_this_epoch = None\n",
    "        self._restored_from_state = False\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataset(cls, dataset: torch.utils.data.Dataset, batch_size: int,\n",
    "                 device: torch.device=None, repeat: bool=False, shuffle: Optional[bool]=None):\n",
    "        return cls(dataset, batch_size, device=device, repeat=repeat, shuffle=shuffle)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_datasets(cls, train_ds: torch.utils.data.Dataset, batch_size: OneorMore[int],\n",
    "                      val_ds: Optional[torch.utils.data.Dataset]=None, test_ds: Optional[torch.utils.data.Dataset]=None,\n",
    "                      device: OneorMore[torch.device]=None, repeat: OneorMore[bool]=False,\n",
    "                      shuffle: Optional[OneorMore[Optional[bool]]]=None) -> Iterable['DefaultLoader']:\n",
    "        n_ds = 1\n",
    "        if val_ds is not None: n_ds += 1\n",
    "        if test_ds is not None: n_ds += 1\n",
    "            \n",
    "        args = (expand(batch_size, n_ds), )\n",
    "        kwargs = {\n",
    "            \"device\": expand(device, n_ds),\n",
    "            \"repeat\": expand(repeat, n_ds),\n",
    "            \"shuffle\": expand(shuffle, n_ds),\n",
    "        }\n",
    "        \n",
    "        i = 0\n",
    "        yield cls.from_dataset(train_ds, *([a[i] for a in args]), **({k: v[i] for k, v in kwargs.items()}))\n",
    "        i += 1\n",
    "        if val_ds is not None:\n",
    "            yield cls.from_dataset(val_ds, *([a[i] for a in args]), **({k: v[i] for k, v in kwargs.items()}))\n",
    "            i += 1\n",
    "        if test_ds is not None:\n",
    "            yield cls.from_dataset(test_ds, *([a[i] for a in args]), **({k: v[i] for k, v in kwargs.items()}))\n",
    "\n",
    "    def _process_batch(self, data: Dict[ColumnName, OneorMore[ArrayLike]]) -> ProcessedBatch:\n",
    "        \"\"\"\n",
    "        Converts examples in a dataset to model inputs by using the fields to transform\n",
    "        the inputs to tensors. Override in subclass to add custom behavior.\n",
    "        \"\"\"\n",
    "        in_data = {}\n",
    "        tgt_data = {}\n",
    "        for k, batch in data.items():\n",
    "            fld = self.dataset.fields[k]\n",
    "            if isinstance(fld, (tuple, list)):\n",
    "                for f, v in zip(fld, batch):\n",
    "                    data_dict = tgt_data if f.is_target else in_data\n",
    "                    if k not in data_dict: data_dict[k] = []\n",
    "                    data_dict[k].append(f.transform_batch(v, device=self.device, train=self.dataset.train))\n",
    "            else:\n",
    "                tsr = fld.transform_batch(batch, device=self.device, train=self.dataset.train)\n",
    "                # add to data dicts\n",
    "                if fld.is_target: tgt_data[k] = tsr\n",
    "                else: in_data[k] = tsr\n",
    "        return in_data, tgt_data\n",
    "            \n",
    "    def _batches(self) -> Iterable[ProcessedBatch]:\n",
    "        \"\"\"\n",
    "        Iterates through the dataset while generating batches of input and target variables.\n",
    "        Assumes dataset can be indexed using a list.\n",
    "        \"\"\"\n",
    "        indices = []\n",
    "        for i in self.index_generator(range(len(self.dataset))):\n",
    "            indices.append(i)\n",
    "            if len(indices) == self.batch_size:\n",
    "                yield self._process_batch(self.dataset[indices])\n",
    "                indices = []\n",
    "        if len(indices) > 0:\n",
    "            yield self._process_batch(self.dataset[indices])    \n",
    "\n",
    "    def init_epoch(self):\n",
    "        \"\"\"Set up the batch generator for a new epoch.\"\"\"\n",
    "        if self.shuffle:\n",
    "            if self._restored_from_state:\n",
    "                self.index_generator.random_state = self._random_state_this_epoch\n",
    "            else:\n",
    "                self._random_state_this_epoch = self.index_generator.random_state\n",
    "        \n",
    "        if self._restored_from_state:\n",
    "            self._restored_from_state = False\n",
    "        else:\n",
    "            self._iterations_this_epoch = 0\n",
    "\n",
    "        if not self.repeat: self.iterations = 0\n",
    "    \n",
    "    @property\n",
    "    def epoch(self):\n",
    "        return math.floor(self.iterations / len(self))\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.dataset) / self.batch_size)\n",
    "\n",
    "    def __iter__(self) -> Iterable[Dict[str, torch.tensor]]:\n",
    "        while True:\n",
    "            self.init_epoch()\n",
    "            for idx, minibatch in enumerate(self._batches()):\n",
    "                # fast-forward if loaded from state\n",
    "                if self._iterations_this_epoch > idx:\n",
    "                    continue\n",
    "                self.iterations += 1\n",
    "                self._iterations_this_epoch += 1\n",
    "                yield minibatch\n",
    "            if not self.repeat:\n",
    "                break\n",
    "\n",
    "    def state_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"iterations\": self.iterations,\n",
    "            \"iterations_this_epoch\": self._iterations_this_epoch,\n",
    "            \"random_state_this_epoch\": self._random_state_this_epoch,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict: Dict[str, Any]):\n",
    "        self.iterations = state_dict[\"iterations\"]\n",
    "        self._iterations_this_epoch = state_dict[\"iterations_this_epoch\"]\n",
    "        self._random_state_this_epoch = state_dict[\"random_state_this_epoch\"]\n",
    "        self._restored_from_state = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment\n",
    "# from torchtable import *\n",
    "# from torchtable.field import *\n",
    "# from torchtable.dataset import *\n",
    "# from torchtable.loader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "from torchtable.field import *\n",
    "from torchtable.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    for v in x:\n",
    "        if isinstance(v, (tuple, list)):\n",
    "            yield from v\n",
    "        else:\n",
    "            yield v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_from_dataset\n",
    "df = pd.DataFrame({\"a\": [1, 2, 3, 4, 5],\n",
    "                   \"b\": [-0.4, -2.1, 3.3, 4.4, 5.5]})\n",
    "ds = TabularDataset.from_df(df, fields={\n",
    "    \"a\": CategoricalField(max_features=100),\n",
    "    \"b\": [NumericField(normalization=\"Gaussian\"), IdentityField()],\n",
    "})\n",
    "dl = DefaultLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_from_datasets\n",
    "df1 = pd.DataFrame({\"a\": [1, 2, 3, 4, 5],\n",
    "                   \"b\": [-0.4, -2.1, 3.3, 4.4, 5.5]})\n",
    "df2 = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [-1., -2, -3.]})\n",
    "df3 = pd.DataFrame({\"a\": [3, 2], \"b\": [-1., -2]})\n",
    "train, val, test = TabularDataset.from_dfs(df1, val_df=df2, test_df=df3, fields={\n",
    "    \"a\": CategoricalField(),\n",
    "    \"b\": [NumericField(normalization=\"Gaussian\"), CategoricalField(handle_unk=True)],\n",
    "})\n",
    "# all present\n",
    "train_dl, val_dl, test_dl = DefaultLoader.from_datasets(train, 3, val_ds=val, test_ds=test)\n",
    "# val only\n",
    "train_dl, val_dl = DefaultLoader.from_datasets(train, 3, val_ds=val, test_ds=None)\n",
    "# test only\n",
    "train_dl, test_dl = DefaultLoader.from_datasets(train, 3, val_ds=None, test_ds=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_from_datasets_multiple_args\n",
    "df1 = pd.DataFrame({\"a\": [3, 4, 5, 1, 2],\n",
    "                   \"b\": [1.3, -2.1, 2.3, 5.4, 5.6]})\n",
    "df2 = pd.DataFrame({\"a\": [1, 2, 3, 4, 5], \"b\": [-1., -2, -3., -4., -5.]})\n",
    "df3 = pd.DataFrame({\"a\": [3, 2], \"b\": [-1., -2]})\n",
    "train, val, test = TabularDataset.from_dfs(df1, val_df=df2, test_df=df3, fields={\n",
    "    \"a\": CategoricalField(),\n",
    "    \"b\": [NumericField(normalization=\"Gaussian\"), CategoricalField(handle_unk=True)],\n",
    "})\n",
    "train_dl, val_dl, test_dl = DefaultLoader.from_datasets(train, (5, 3, 2), val_ds=val, test_ds=test,\n",
    "                                                        device=(None, None, None), repeat=(True, True, True),\n",
    "                                                        shuffle=(True, True, True))\n",
    "x, y = next(iter(train_dl))\n",
    "for v in flatten(itertools.chain(x.values(), y.values())): assert v.size()[0] == 5\n",
    "x, y = next(iter(val_dl))\n",
    "for v in flatten(itertools.chain(x.values(), y.values())): assert v.size()[0] == 3\n",
    "x, y = next(iter(test_dl))\n",
    "for v in flatten(itertools.chain(x.values(), y.values())): assert v.size()[0] == 2\n",
    "    \n",
    "train_dl, val_dl = DefaultLoader.from_datasets(train, (3, 4), val_ds=val, test_ds=None)\n",
    "x, y = next(iter(train_dl))\n",
    "for v in flatten(itertools.chain(x.values(), y.values())): assert v.size()[0] == 3\n",
    "x, y = next(iter(val_dl))\n",
    "for v in flatten(itertools.chain(x.values(), y.values())): assert v.size()[0] == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_real_data\n",
    "\"\"\"Smoke test for real dataset\"\"\"\n",
    "df = pd.read_csv(\"./tests/resources/sample.csv\")\n",
    "ds = TabularDataset.from_df(df, fields={\n",
    "    \"category_1\": None,\n",
    "    \"category_3\": None,\n",
    "    \"merchant_id\": None,\n",
    "    \"subsector_id\": CategoricalField(min_freq=3),\n",
    "    \"merchant_category_id\": CategoricalField(min_freq=3),\n",
    "    \"city_id\": None,\n",
    "    \"month_lag\": NumericField(normalization=\"RankGaussian\"),\n",
    "    \"card_id\": None,\n",
    "    \"installments\": NumericField(normalization=None),\n",
    "    \"state_id\": CategoricalField(),\n",
    "    \"category_2\": NumericField(normalization=None),\n",
    "    \"authorized_flag\": CategoricalField(min_freq=3, handle_unk=True),\n",
    "    \"purchase_date\": datetime_fields(),\n",
    "    \"purchase_amount\": NumericField(normalization=None, fill_missing=None, is_target=True),\n",
    "}, train=True)\n",
    "\n",
    "bs = 32\n",
    "x, y = next(iter(DefaultLoader.from_dataset(ds, bs)))\n",
    "for v in flatten(itertools.chain(x.values(), y.values())):\n",
    "    assert v.size()[0] == bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
