{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset/core.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "import sys; sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace(torchtable, ..custom_types)\n",
    "from torchtable import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace(torchtable, .)\n",
    "from torchtable.utils import with_default, flat_filter, apply_oneormore, fold_oneormore\n",
    "from torchtable.operator import Operator, LambdaOperator, FillMissing, Categorize, Normalize\n",
    "from torchtable.field import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the meat of the implementaion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FieldOrFields = Union[Field, FieldCollection, Collection[Field]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FieldDict(dict):\n",
    "    \"\"\"Dictionary with additional methods. Intended for internal use with the TabularDataset.\"\"\"\n",
    "    def flatmap(self, func: Callable[[str, Field], Any], with_index=False):\n",
    "        for k, v in self.items():\n",
    "            if isinstance(v, FieldCollection):\n",
    "                for (i, f) in enumerate(v):\n",
    "                    if with_index: yield func(key, f, i)\n",
    "                    else: yield func(key, f)\n",
    "            else:\n",
    "                if with_index: yield func(x, -1) \n",
    "                else: yield func(key, x)   \n",
    "\n",
    "    def flatfilter(self, predicate: Callable[[str, Field], bool]):\n",
    "        for k, fld in self.items():\n",
    "            if isinstance(fld, FieldCollection):\n",
    "                for f in fld:\n",
    "                    if predicate(k, f): yield  f\n",
    "            else:\n",
    "                if predicate(k, fld): yield fld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset for tabular data.\n",
    "    Args:\n",
    "        fields: A dictionary mapping from a column/columns in the raw data to a Field/Fields.\n",
    "                To specify multiple columns as input, use a tuple of column names.\n",
    "                To map a single column to multiple fields, use a list of fields.\n",
    "                Each field will be mapped to a single entry in the processed dataset.\n",
    "        train: Whether this dataset is the training set. This affects whether the fields will fit the given data.\n",
    "    \"\"\"\n",
    "    def __init__(self, examples: Dict[ColumnName, OneorMore[ArrayLike]],\n",
    "                 fields: FieldDict, train=True):\n",
    "        \"\"\"This constructor is not intended to be called directly.\"\"\"\n",
    "        self.examples = examples\n",
    "        example = next(iter(self.examples.values()))\n",
    "        self.length = fold_oneormore(lambda x,y: len(y), example, [])\n",
    "        self.fields = fields\n",
    "        self.train = train\n",
    "        self.continuous_fields = list(self.fields.flatfilter(lambda _, x: x.continuous and not x.is_target))\n",
    "        self.categorical_fields = list(self.fields.flatfilter(lambda _, x: x.categorical and not x.is_target))\n",
    "        self.target_fields = list(self.fields.flatfilter(lambda _, x: x.is_target))\n",
    "                                  \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def _index_example(self, k: ColumnName, val: OneorMore[ArrayLike], idx) -> OneorMore[ArrayLike]:\n",
    "        # check if the field is a tuple/list since the field output might be a list\n",
    "        # even though the field itself is not\n",
    "        if isinstance(self.fields[k], (tuple, list)):\n",
    "            return [v[idx] for v in val]\n",
    "        else:\n",
    "            return val[idx]\n",
    "    \n",
    "    def __getitem__(self, idx) -> Dict[str, ArrayLike]:\n",
    "        return {k: self.fields[k].index(v, idx) for k, v in self.examples.items()}\n",
    "    \n",
    "    def __repr__(self):\n",
    "        fields_rep = \",\\n\".join([\" \" * 4 + str(x) for x in self.fields.values()])\n",
    "        nl = \"\\n\"\n",
    "        return f\"TabularDataset({nl + fields_rep + nl})\"\n",
    "        \n",
    "    @classmethod\n",
    "    def from_df(cls, df: pd.DataFrame, fields: Dict[ColumnName, FieldOrFields],\n",
    "                train=True) -> 'TabularDataset':\n",
    "        \"\"\"\n",
    "        Initialize a dataset from a pandas dataframe.\n",
    "        Args:\n",
    "            df: pandas dataframe to initialize from\n",
    "\n",
    "            fields: Dictionary mapping from a column identifier to a field or fields.\n",
    "            The key can be a single column name or a tuple of multiple columns. The column(s) specified by the key will be passed to the field(s) transform method.\n",
    "            The value can be a single field, a list/tuple of fields, or a `field.FieldCollection`.\n",
    "            In general, each example in the dataset will mirror the structure of the fields passed.\n",
    "            For instance, if you pass multiple fields for a certain key, the example will also have multiple outputs for the given key structured as a list.\n",
    "            If you want a flat dictionary for the example, consider using the `flatten` attribute in the `field.FieldCollection` class\n",
    "            (see `field.FieldCollection` documentation for more details).\n",
    "        Kwargs:\n",
    "            train: Whether this dataset is the training set. This affects whether the fields will fit the given data.\n",
    "        Example:\n",
    "        >>> df.head(2)\n",
    "                  authorized_flag          card_id  price\n",
    "        0               Y  C_ID_4e6213e9bc       1.2\n",
    "        1               Y  C_ID_4e6213e9bc       3.4\n",
    "        >>> ds = TabularDataset.from_df(df, fields={\n",
    "        ...     \"authorized_flag\": CategoricalField(handle_unk=False), # standard field\n",
    "        ...     \"card_id\": [CategoricalField(handle_unk=True),\n",
    "        ...                 Field(LambdaOperator(lambda x: x.str[0]) > Categorize())], # multiple fields and custom fields\n",
    "        ...     \"price\": NumericField(fill_missing=None, normalization=None, is_target=True), # target field\n",
    "        ...     (\"authorized_flag\", \"price\"): Field(LambdaOperator(\n",
    "        ...             lambda x: (x[\"authorized_flag\"] == \"N\").astype(\"int\") * x[\"price\"])), # multiple column field\n",
    "        ... })\n",
    "        >>> ds[0] \n",
    "        {\"authorized_flag\": 0,\n",
    "         \"card_id\": [1, 0],\n",
    "          \"price\": 1.2,\n",
    "          (\"authorized_flag\", \"price\"): 0.}\n",
    "        \"\"\"\n",
    "        missing_cols = set(df.columns) - set(fields.keys())\n",
    "        if len(missing_cols) > 0:\n",
    "            logger.warning(f\"The following columns are missing from the fields list: {missing_cols}\")\n",
    "        \n",
    "        # convert raw lists/tuples of fields to FieldCollections\n",
    "        for k, v in fields.items():\n",
    "            if type(v) in (tuple, list): fields[k] = FieldCollection(*v)\n",
    "        \n",
    "        def _to_df_key(k):\n",
    "            # change tuples to lists for accessing dataframe columns\n",
    "            # this is necessary since lists cannot be dictionary keys\n",
    "            if isinstance(k, tuple): return list(k)\n",
    "            else: return k\n",
    "\n",
    "        # construct examples while setting names\n",
    "        final_dataset_fields = FieldDict()\n",
    "        examples = {}            \n",
    "        for k, fld in fields.items():\n",
    "            if fld is None: continue\n",
    "            # fields are either a Field or FieldCollection, so the following code works\n",
    "            fld.name = k\n",
    "            if isinstance(fld, FieldCollection) and fld.flatten:\n",
    "                # construct a flat representation\n",
    "                for sub_fld, output in zip(fld, fld.transform(df[_to_df_key(k)], train=train)):\n",
    "                    examples[sub_fld.name] = output\n",
    "                    final_dataset_fields[sub_fld.name] = sub_fld\n",
    "            else:\n",
    "                examples[k] = fld.transform(df[_to_df_key(k)], train=train)\n",
    "                final_dataset_fields[k] = fld\n",
    "        \n",
    "        return cls(examples, final_dataset_fields, train=train)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dfs(cls, train_df: pd.DataFrame, \n",
    "                 val_df: pd.DataFrame=None, test_df: pd.DataFrame=None,\n",
    "                 fields: Dict[ColumnName, FieldOrFields]=None) -> Iterable['TabularDataset']:\n",
    "        \"\"\"\n",
    "        Generates datasets from train, val, and test dataframes.\n",
    "        Example:\n",
    "        >>> trn, val, test = TabularDataset.from_dfs(train_df, val_df=val_df, test_df=test_df, fields={\n",
    "        ...   \"a\": NumericalField(), \"b\": CategoricalField(),\n",
    "        ...  })\n",
    "        \"\"\"\n",
    "        train = cls.from_df(train_df, fields, train=True)\n",
    "        yield train\n",
    "        if val_df is not None:\n",
    "            yield cls.from_df(val_df, fields, train=False)\n",
    "        if test_df is not None:\n",
    "            # remove all target fields\n",
    "            non_target_fields = {}\n",
    "                \n",
    "            for k, fld in fields.items():\n",
    "                if fld is None: continue\n",
    "                if isinstance(fld, (tuple, list)):\n",
    "                    non_target_fields[k] = []\n",
    "                    for f in fld:\n",
    "                        if not f.is_target: non_target_fields[k].append(f)\n",
    "                    if len(non_target_fields[k]) == 0: non_target_fields[k] = None\n",
    "                else:\n",
    "                    if not fld.is_target:\n",
    "                        non_target_fields[k] = fld\n",
    "                    else:\n",
    "                        non_target_fields[k] = None\n",
    "            yield cls.from_df(test_df, non_target_fields, train=False)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_csv(cls, fname: str, fields: Dict[ColumnName, FieldOrFields],\n",
    "                 train=True, csv_read_params: dict={}) -> 'TabularDataset':\n",
    "        \"\"\"\n",
    "        Initialize a dataset from a csv file. See documentation on `TabularDataset.from_df` for more details on arguments.\n",
    "        Kwargs:\n",
    "            csv_read_params: Keyword arguments to pass to the `pd.read_csv` method.\n",
    "        \"\"\"\n",
    "        return cls.from_df(pd.read_csv(fname, **csv_read_params), fields=fields, train=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment\n",
    "# from torchtable import *\n",
    "# from torchtable.field import *\n",
    "# from torchtable.operator import *\n",
    "# from torchtable.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_basic\n",
    "df = pd.DataFrame({\"a\": [50, 40, 30, 20, 10],\n",
    "                   \"b\": [-0.4, -2.1, 3.3, 4.4, 5.5]})\n",
    "ds = TabularDataset.from_df(df, fields={\n",
    "    \"a\": CategoricalField(),\n",
    "    \"b\": NumericField(fill_missing=None, normalization=None),\n",
    "})\n",
    "assert len(ds) == len(df)\n",
    "for i in range(len(ds)):\n",
    "    example = ds[i]\n",
    "    assert \"a\" in example\n",
    "    assert \"b\" in example\n",
    "    assert example[\"a\"] != df.iloc[i][\"a\"]\n",
    "    assert example[\"b\"] == df.iloc[i][\"b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_multiple_fields\n",
    "df = pd.DataFrame({\"a\": [1, 2, 3, 4, 5],\n",
    "                   \"b\": [-0.4, -2.1, 3.3, 4.4, 5.5], \n",
    "                   \"c\": [1, 1, 1, 1, 1]})\n",
    "ds = TabularDataset.from_df(df, fields={\n",
    "    \"a\": CategoricalField(max_features=100),\n",
    "    \"b\": [NumericField(normalization=\"Gaussian\"), Field(LambdaOperator(lambda x: x * 2))],\n",
    "    \"c\": None,\n",
    "})\n",
    "assert len(ds) == len(df)\n",
    "assert len(ds.fields) == 2\n",
    "for i in range(len(ds)):\n",
    "    example = ds[i]\n",
    "    assert \"a\" in example\n",
    "    assert \"b\" in example\n",
    "    assert len(example) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_index_with_list\n",
    "df = pd.DataFrame({\"a\": [1, 2, 3, 4, 5],\n",
    "                   \"b\": [-0.4, -2.1, 3.3, 4.4, 5.5]})\n",
    "ds = TabularDataset.from_df(df, fields={\n",
    "    \"a\": CategoricalField(max_features=100),\n",
    "    \"b\": [NumericField(normalization=\"Gaussian\"), IdentityField()],\n",
    "})\n",
    "assert len(ds.fields) == 2\n",
    "list_idx = [0, 1, 3, 4]\n",
    "examples = ds[list_idx]\n",
    "assert len(examples) == 2\n",
    "assert len(examples[\"a\"]) == 4\n",
    "assert len(examples[\"b\"]) == 2\n",
    "assert len(examples[\"b\"][0]) == 4\n",
    "assert len(examples[\"b\"][1]) == 4\n",
    "assert (examples[\"b\"][1].values == df.iloc[list_idx][\"b\"].values).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_from_dfs\n",
    "df1 = pd.DataFrame({\"a\": [1, 2, 3, 4, 5],\n",
    "                   \"b\": [-0.4, -2.1, 3.3, 4.4, 5.5]})\n",
    "df2 = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [-1., -2, -3.]})\n",
    "df3 = pd.DataFrame({\"a\": [3, 2], \"b\": [-1., -2]})\n",
    "# all present\n",
    "train, val, test = TabularDataset.from_dfs(df1, val_df=df2, test_df=df3, fields={\n",
    "    \"a\": CategoricalField(),\n",
    "    \"b\": [NumericField(normalization=\"Gaussian\"), CategoricalField(handle_unk=True)],\n",
    "})\n",
    "# only train and val\n",
    "train, val = TabularDataset.from_dfs(df1, val_df=df2, test_df=None, fields={\n",
    "    \"a\": CategoricalField(),\n",
    "    \"b\": [NumericField(normalization=\"Gaussian\"), CategoricalField(handle_unk=True)],\n",
    "})\n",
    "# only train and test\n",
    "train, test = TabularDataset.from_dfs(df1, val_df=None, test_df=df3, fields={\n",
    "    \"a\": CategoricalField(),\n",
    "    \"b\": [NumericField(normalization=\"Gaussian\"), CategoricalField(handle_unk=True)],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_from_dfs_with_target\n",
    "df1 = pd.DataFrame({\"a\": [1, 2, 3, 4, 5],\n",
    "                   \"b\": [-0.4, -2.1, 3.3, 4.4, 5.5]})\n",
    "df2 = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [-1., -2, -3.]})\n",
    "df3 = pd.DataFrame({\"a\": [3, 2], \"b\": [-1., -2]})\n",
    "train, val, test = TabularDataset.from_dfs(df1, val_df=df2, test_df=df3, fields={\n",
    "    \"a\": CategoricalField(is_target=True),\n",
    "    \"b\": [NumericField(normalization=\"Gaussian\"), CategoricalField(handle_unk=True)],\n",
    "})\n",
    "train, val, test = TabularDataset.from_dfs(df1, val_df=df2, test_df=df3, fields={\n",
    "    \"a\": CategoricalField(),\n",
    "    \"b\": [NumericField(normalization=\"Gaussian\", is_target=True), CategoricalField(handle_unk=True)],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_real_data\n",
    "df = pd.read_csv(\"./tests/resources/sample.csv\")\n",
    "ds = TabularDataset.from_df(df, fields={\n",
    "    \"category_1\": None,\n",
    "    \"category_3\": None,\n",
    "    \"merchant_id\": None,\n",
    "    \"subsector_id\": CategoricalField(min_freq=3),\n",
    "    \"merchant_category_id\": CategoricalField(min_freq=3),\n",
    "    \"city_id\": None,\n",
    "    \"month_lag\": NumericField(normalization=\"RankGaussian\"),\n",
    "    \"card_id\": None,\n",
    "    \"installments\": NumericField(normalization=None),\n",
    "    \"state_id\": CategoricalField(),\n",
    "    \"category_2\": NumericField(normalization=None),\n",
    "    \"authorized_flag\": CategoricalField(min_freq=3, handle_unk=True),\n",
    "    \"purchase_date\": datetime_fields(),\n",
    "    \"purchase_amount\": NumericField(normalization=None, fill_missing=None, is_target=True),\n",
    "}, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_real_data_csv\n",
    "ds = TabularDataset.from_csv(\"./tests/resources/sample.csv\", {\n",
    "    \"category_1\": None,\n",
    "    \"category_3\": None,\n",
    "    \"merchant_id\": None,\n",
    "    \"subsector_id\": CategoricalField(min_freq=3),\n",
    "    \"merchant_category_id\": CategoricalField(min_freq=3),\n",
    "    \"city_id\": None,\n",
    "    \"month_lag\": NumericField(normalization=\"RankGaussian\"),\n",
    "    \"card_id\": None,\n",
    "    \"installments\": NumericField(normalization=None),\n",
    "    \"state_id\": CategoricalField(),\n",
    "    \"category_2\": NumericField(normalization=None),\n",
    "    \"authorized_flag\": CategoricalField(min_freq=3, handle_unk=True),\n",
    "    \"purchase_date\": datetime_fields(),\n",
    "    \"purchase_amount\": NumericField(normalization=None, fill_missing=None, is_target=True),\n",
    "}, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_multiple_cols\n",
    "df1 = pd.DataFrame({\"a\": [1, 2, 3, 4, 5],\n",
    "                   \"b\": [-0.4, -2.1, 3.3, 4.4, 5.5]})\n",
    "train = TabularDataset.from_df(df1, fields={\n",
    "    \"a\": CategoricalField(is_target=True),\n",
    "    \"b\": [NumericField(normalization=\"Gaussian\"), CategoricalField(handle_unk=True)],\n",
    "    (\"a\", \"b\"): Field(LambdaOperator(lambda x: x[\"a\"] + x[\"b\"])),\n",
    "})\n",
    "np.testing.assert_allclose(train.examples[(\"a\", \"b\")].values, (df1[\"a\"] + df1[\"b\"]).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_fieldcollection\n",
    "\"\"\"Smoke test to confirm FieldCollection works with TabularDataset\"\"\"\n",
    "df = pd.DataFrame({\"a\": [1, 2, 3, 4, 5],\n",
    "                   \"b\": [-0.4, -2.1, 3.3, 4.4, 5.5]})\n",
    "ds = TabularDataset.from_df(df, fields={\n",
    "    \"a\": CategoricalField(max_features=100),\n",
    "    \"b\": FieldCollection(NumericField(normalization=\"Gaussian\"), Field(LambdaOperator(lambda x: x * 2))),\n",
    "})\n",
    "assert len(ds) == len(df)\n",
    "assert len(ds.fields) == 2\n",
    "for i in range(len(ds)):\n",
    "    example = ds[i]\n",
    "    assert \"a\" in example\n",
    "    assert \"b\" in example\n",
    "    assert len(example) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_fieldcollection_flatten\n",
    "df = pd.DataFrame({\"a\": [1, 2, 3, 4, 5],\n",
    "                   \"b\": [-0.4, -2.1, 3.3, 4.4, 5.5]})\n",
    "ds = TabularDataset.from_df(df, fields={\n",
    "    \"a\": CategoricalField(max_features=100),\n",
    "    \"b\": FieldCollection(NumericField(normalization=\"Gaussian\"), Field(LambdaOperator(lambda x: x * 2)), flatten=True),\n",
    "})\n",
    "assert len(ds) == len(df)\n",
    "assert len(ds.fields) == 3\n",
    "for i in range(len(ds)):\n",
    "    example = ds[i]\n",
    "    assert \"a\" in example\n",
    "    assert \"b\" not in example\n",
    "    assert len(example) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
