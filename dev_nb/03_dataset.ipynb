{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset/core.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "import sys; sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace(torchtable, ..custom_types)\n",
    "from torchtable import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace(torchtable, .)\n",
    "from torchtable.utils import with_default, flat_filter\n",
    "from torchtable.operator import Operator, LambdaOperator, FillMissing, Categorize, Normalize\n",
    "from torchtable.field import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the meat of the implementaion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset for tabular data.\n",
    "    Args:\n",
    "        fields: A dictionary mapping from a column/columns in the raw data to a Field/Fields.\n",
    "                To specify multiple columns as input, use a tuple of column names.\n",
    "                To map a single column to multiple fields, use a list of fields.\n",
    "                Each field will be mapped to a single entry in the processed dataset.\n",
    "        train: Whether this dataset is the training set. This affects whether the fields will fit the given data.\n",
    "    Example:\n",
    "        >>> df.head(2)\n",
    "                  authorized_flag          card_id  price\n",
    "        0               Y  C_ID_4e6213e9bc       1.2\n",
    "        1               Y  C_ID_4e6213e9bc       3.4\n",
    "        >>> TabularDataset.from_df(df, fields={\n",
    "        ...     \"authorized_flag\": CategoricalField(handle_unk=False), # standard field\n",
    "        ...     \"card_id\": [CategoricalField(),\n",
    "        ...                 Field(LambdaOperator(lambda x: x.str[0]) > Categorize())], # multiple fields and custom fields\n",
    "        ...     \"price\": NumericalField(fill_missing=None, normalization=None, is_target=True), # target field\n",
    "        ...     (\"authorized_flag\", \"price\"): IdentityField(), # multiple column field\n",
    "        ... })\n",
    "    \"\"\"\n",
    "    def __init__(self, examples: Dict[ColumnName, OneorMore[ArrayLike]],\n",
    "                 fields: Dict[ColumnName, OneorMore[Field]], train=True):\n",
    "        self.examples = examples\n",
    "        # all fields should be of the same length\n",
    "        self.length = len(next(iter(self.examples.values())))\n",
    "        self.fields = fields\n",
    "        self.train = train\n",
    "        self.continuous_fields = list(flat_filter(fields.values(), lambda x: x.continuous and not x.is_target))\n",
    "        self.categorical_fields = list(flat_filter(fields.values(), lambda x: x.categorical and not x.is_target))\n",
    "        self.target_fields = list(flat_filter(fields.values(), lambda x: x.is_target))\n",
    "                                  \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def _index_example(self, k: ColumnName, val: OneorMore[ArrayLike], idx) -> OneorMore[ArrayLike]:\n",
    "        if isinstance(self.fields[k], (tuple, list)):\n",
    "            return [v[idx] for v in val]\n",
    "        else:\n",
    "            return val[idx]\n",
    "    \n",
    "    def __getitem__(self, idx) -> Dict[str, ArrayLike]:\n",
    "        return {k: self._index_example(k, v, idx) for k, v in self.examples.items()}\n",
    "    \n",
    "    def __repr__(self):\n",
    "        fields_rep = \",\\n\".join([\" \" * 4 + str(x) for x in self.fields.values()])\n",
    "        nl = \"\\n\"\n",
    "        return f\"TabularDataset({nl + fields_rep + nl})\"\n",
    "        \n",
    "    @classmethod\n",
    "    def from_df(cls, df: pd.DataFrame, fields: Dict[ColumnName, OneorMore[Field]],\n",
    "                train=True) -> 'TabularDataset':\n",
    "        \"\"\"Initialize a dataset from a pandas dataframe.\"\"\"\n",
    "        def _to_df_key(k):\n",
    "            if isinstance(k, tuple): return list(k)\n",
    "            else: return k\n",
    "        missing_cols = set(df.columns) - set(fields.keys())\n",
    "        if len(missing_cols) > 0:\n",
    "            logger.warning(f\"The following columns are missing from the fields list: {missing_cols}\")\n",
    "        \n",
    "        examples = {}\n",
    "        for k, fld in fields.items():\n",
    "            if fld is None: continue\n",
    "            if isinstance(fld, (tuple, list)):\n",
    "                # if multiple fields are specified, hook them all to the same column\n",
    "                examples[k] = []\n",
    "                for i, f in enumerate(fld):\n",
    "                    f.name = with_default(f.name, f\"{k}_{i}\")\n",
    "                    examples[k].append(f.transform(df[_to_df_key(k)], train=train))\n",
    "            else:\n",
    "                fld.name = with_default(fld.name, k)\n",
    "                examples[k] = fld.transform(df[_to_df_key(k)], train=train)\n",
    "        \n",
    "        return cls(examples, {k: v for k, v in fields.items() if v is not None}, train=train)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dfs(cls, train_df: pd.DataFrame, \n",
    "                 val_df: pd.DataFrame=None, test_df: pd.DataFrame=None,\n",
    "                 fields: Dict[ColumnName, OneorMore[Field]]=None) -> Iterable['TabularDataset']:\n",
    "        \"\"\"\n",
    "        Generates datasets from train, val, and test dataframes.\n",
    "        Example:\n",
    "        >>> trn, val, test = TabularDataset.from_dfs(train_df, val_df=val_df, test_df=test_df, fields={\n",
    "        ...   \"a\": NumericalField(), \"b\": CategoricalField(),\n",
    "        ...  })\n",
    "        \"\"\"\n",
    "        train = cls.from_df(train_df, fields, train=True)\n",
    "        yield train\n",
    "        if val_df is not None:\n",
    "            yield cls.from_df(val_df, fields, train=False)\n",
    "        if test_df is not None:\n",
    "            # remove all target fields\n",
    "            non_target_fields = {}\n",
    "            for k, fld in fields.items():\n",
    "                if fld is None: continue\n",
    "                if isinstance(fld, (tuple, list)):\n",
    "                    non_target_fields[k] = []\n",
    "                    for f in fld:\n",
    "                        if not f.is_target: non_target_fields[k].append(f)\n",
    "                    if len(non_target_fields[k]) == 0: non_target_fields[k] = None\n",
    "                else:\n",
    "                    if not fld.is_target:\n",
    "                        non_target_fields[k] = fld\n",
    "                    else:\n",
    "                        non_target_fields[k] = None\n",
    "            yield cls.from_df(test_df, non_target_fields, train=False)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_csv(cls, fname: str, fields: Dict[ColumnName, OneorMore[Field]],\n",
    "                 train=True, csv_read_params: dict={}) -> 'TabularDataset':\n",
    "        \"\"\"\n",
    "        Initialize a dataset from a csv file.\n",
    "        Kwargs:\n",
    "            csv_read_params: Keyword arguments to pass to the `pd.read_csv` method.\n",
    "        \"\"\"\n",
    "        return cls.from_df(pd.read_csv(fname, **csv_read_params), fields=fields, train=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment\n",
    "# from torchtable import *\n",
    "# from torchtable.field import *\n",
    "# from torchtable.operator import *\n",
    "# from torchtable.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_basic\n",
    "df = pd.DataFrame({\"a\": [50, 40, 30, 20, 10],\n",
    "                   \"b\": [-0.4, -2.1, 3.3, 4.4, 5.5]})\n",
    "ds = TabularDataset.from_df(df, fields={\n",
    "    \"a\": CategoricalField(),\n",
    "    \"b\": NumericField(fill_missing=None, normalization=None),\n",
    "})\n",
    "assert len(ds) == len(df)\n",
    "for i in range(len(ds)):\n",
    "    example = ds[i]\n",
    "    assert \"a\" in example\n",
    "    assert \"b\" in example\n",
    "    assert example[\"a\"] != df.iloc[i][\"a\"]\n",
    "    assert example[\"b\"] == df.iloc[i][\"b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_multiple_fields\n",
    "df = pd.DataFrame({\"a\": [1, 2, 3, 4, 5],\n",
    "                   \"b\": [-0.4, -2.1, 3.3, 4.4, 5.5], \n",
    "                   \"c\": [1, 1, 1, 1, 1]})\n",
    "ds = TabularDataset.from_df(df, fields={\n",
    "    \"a\": CategoricalField(max_features=100),\n",
    "    \"b\": [NumericField(normalization=\"Gaussian\"), Field(LambdaOperator(lambda x: x * 2))],\n",
    "    \"c\": None,\n",
    "})\n",
    "assert len(ds) == len(df)\n",
    "assert len(ds.fields) == 2\n",
    "for i in range(len(ds)):\n",
    "    example = ds[i]\n",
    "    assert \"a\" in example\n",
    "    assert \"b\" in example\n",
    "    assert len(example) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_index_with_list\n",
    "df = pd.DataFrame({\"a\": [1, 2, 3, 4, 5],\n",
    "                   \"b\": [-0.4, -2.1, 3.3, 4.4, 5.5]})\n",
    "ds = TabularDataset.from_df(df, fields={\n",
    "    \"a\": CategoricalField(max_features=100),\n",
    "    \"b\": [NumericField(normalization=\"Gaussian\"), IdentityField()],\n",
    "})\n",
    "assert len(ds.fields) == 2\n",
    "list_idx = [0, 1, 3, 4]\n",
    "examples = ds[list_idx]\n",
    "assert len(examples) == 2\n",
    "assert len(examples[\"a\"]) == 4\n",
    "assert len(examples[\"b\"]) == 2\n",
    "assert len(examples[\"b\"][0]) == 4\n",
    "assert len(examples[\"b\"][1]) == 4\n",
    "assert (examples[\"b\"][1].values == df.iloc[list_idx][\"b\"].values).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_from_dfs\n",
    "df1 = pd.DataFrame({\"a\": [1, 2, 3, 4, 5],\n",
    "                   \"b\": [-0.4, -2.1, 3.3, 4.4, 5.5]})\n",
    "df2 = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [-1., -2, -3.]})\n",
    "df3 = pd.DataFrame({\"a\": [3, 2], \"b\": [-1., -2]})\n",
    "# all present\n",
    "train, val, test = TabularDataset.from_dfs(df1, val_df=df2, test_df=df3, fields={\n",
    "    \"a\": CategoricalField(),\n",
    "    \"b\": [NumericField(normalization=\"Gaussian\"), CategoricalField(handle_unk=True)],\n",
    "})\n",
    "# only train and val\n",
    "train, val = TabularDataset.from_dfs(df1, val_df=df2, test_df=None, fields={\n",
    "    \"a\": CategoricalField(),\n",
    "    \"b\": [NumericField(normalization=\"Gaussian\"), CategoricalField(handle_unk=True)],\n",
    "})\n",
    "# only train and test\n",
    "train, test = TabularDataset.from_dfs(df1, val_df=None, test_df=df3, fields={\n",
    "    \"a\": CategoricalField(),\n",
    "    \"b\": [NumericField(normalization=\"Gaussian\"), CategoricalField(handle_unk=True)],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_from_dfs_with_target\n",
    "df1 = pd.DataFrame({\"a\": [1, 2, 3, 4, 5],\n",
    "                   \"b\": [-0.4, -2.1, 3.3, 4.4, 5.5]})\n",
    "df2 = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [-1., -2, -3.]})\n",
    "df3 = pd.DataFrame({\"a\": [3, 2], \"b\": [-1., -2]})\n",
    "train, val, test = TabularDataset.from_dfs(df1, val_df=df2, test_df=df3, fields={\n",
    "    \"a\": CategoricalField(is_target=True),\n",
    "    \"b\": [NumericField(normalization=\"Gaussian\"), CategoricalField(handle_unk=True)],\n",
    "})\n",
    "train, val, test = TabularDataset.from_dfs(df1, val_df=df2, test_df=df3, fields={\n",
    "    \"a\": CategoricalField(),\n",
    "    \"b\": [NumericField(normalization=\"Gaussian\", is_target=True), CategoricalField(handle_unk=True)],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_real_data\n",
    "df = pd.read_csv(\"./tests/resources/sample.csv\")\n",
    "ds = TabularDataset.from_df(df, fields={\n",
    "    \"category_1\": None,\n",
    "    \"category_3\": None,\n",
    "    \"merchant_id\": None,\n",
    "    \"subsector_id\": CategoricalField(min_freq=3),\n",
    "    \"merchant_category_id\": CategoricalField(min_freq=3),\n",
    "    \"city_id\": None,\n",
    "    \"month_lag\": NumericField(normalization=\"RankGaussian\"),\n",
    "    \"card_id\": None,\n",
    "    \"installments\": NumericField(normalization=None),\n",
    "    \"state_id\": CategoricalField(),\n",
    "    \"category_2\": NumericField(normalization=None),\n",
    "    \"authorized_flag\": CategoricalField(min_freq=3, handle_unk=True),\n",
    "    \"purchase_date\": datetime_fields(),\n",
    "    \"purchase_amount\": NumericField(normalization=None, fill_missing=None, is_target=True),\n",
    "}, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_real_data_csv\n",
    "ds = TabularDataset.from_csv(\"./tests/resources/sample.csv\", {\n",
    "    \"category_1\": None,\n",
    "    \"category_3\": None,\n",
    "    \"merchant_id\": None,\n",
    "    \"subsector_id\": CategoricalField(min_freq=3),\n",
    "    \"merchant_category_id\": CategoricalField(min_freq=3),\n",
    "    \"city_id\": None,\n",
    "    \"month_lag\": NumericField(normalization=\"RankGaussian\"),\n",
    "    \"card_id\": None,\n",
    "    \"installments\": NumericField(normalization=None),\n",
    "    \"state_id\": CategoricalField(),\n",
    "    \"category_2\": NumericField(normalization=None),\n",
    "    \"authorized_flag\": CategoricalField(min_freq=3, handle_unk=True),\n",
    "    \"purchase_date\": datetime_fields(),\n",
    "    \"purchase_amount\": NumericField(normalization=None, fill_missing=None, is_target=True),\n",
    "}, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_multiple_cols\n",
    "df1 = pd.DataFrame({\"a\": [1, 2, 3, 4, 5],\n",
    "                   \"b\": [-0.4, -2.1, 3.3, 4.4, 5.5]})\n",
    "train = TabularDataset.from_df(df1, fields={\n",
    "    \"a\": CategoricalField(is_target=True),\n",
    "    \"b\": [NumericField(normalization=\"Gaussian\"), CategoricalField(handle_unk=True)],\n",
    "    (\"a\", \"b\"): Field(LambdaOperator(lambda x: x[\"a\"] + x[\"b\"])),\n",
    "})\n",
    "np.testing.assert_allclose(train.examples[(\"a\", \"b\")].values, (df1[\"a\"] + df1[\"b\"]).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
